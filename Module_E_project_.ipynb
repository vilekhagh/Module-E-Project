{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "deaed606",
      "metadata": {
        "id": "deaed606"
      },
      "source": [
        "# E-commerce Product Recommendation System\n",
        "\n",
        "Project track - AI for Market Trend Analysis\n",
        "\n",
        "Problem statement - Implement a product recommendation system.The system Suggests relevant product recomendations to user by combining multiple recommendation strategies and then by employing a hybrid re ranker.\n",
        "\n",
        "Relevance- E-commerce Product recomendations systems serve key use to both users and business. They improve CX for users and get good conversion and retention for business, choosing the besst performative model helps in suggesting most relevant products to user.\n",
        "\n",
        "\n",
        "\n",
        "Run this notebook top-to-bottom. It includes:\n",
        "- Synthetic data generation\n",
        "- Temporal split + feature engineering\n",
        "- Popularity, Item-Item CF, Content TF-IDF, and Hybrid reranker models\n",
        "- Evaluation (Precision/Recall/NDCG/MAP/Coverage)\n",
        "- Demo recommendations + similar-items demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Imports and settings"
      ],
      "metadata": {
        "id": "GlNf5R1rcGSw"
      },
      "id": "GlNf5R1rcGSw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51b2232",
      "metadata": {
        "id": "b51b2232"
      },
      "outputs": [],
      "source": [
        "# 1) Imports & settings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple, Set\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import math\n",
        "import time\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85b3fcc9",
      "metadata": {
        "id": "85b3fcc9"
      },
      "source": [
        "## 2) Configuration\n",
        "\n",
        "improvements-Denser interactions, stronger implicit weights, and more concentrated user preferences.\n",
        "\n",
        "Data source is a synthetic ecommerce data for 2000  users ,1000 items and 300 average interactions per user. these interactions include viewing, adding to cart, purchasing(in increasing order of priority/weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9869c741",
      "metadata": {
        "id": "9869c741",
        "outputId": "c54ab5d9-ff1e-48de-dd5c-187cecb38143",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  Users: 2000, Items: 1000\n",
            "  Interactions: 600000 (300 per user)\n",
            "  Preference strength: 88%\n",
            "  Purchase concentration: 95%\n"
          ]
        }
      ],
      "source": [
        "# 2) Configuration - key improvements\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class CFG:\n",
        "    seed: int = 42\n",
        "\n",
        "    # Denser data: more interactions per user\n",
        "    n_users: int = 2000\n",
        "    n_items: int = 1000\n",
        "    n_categories: int = 20\n",
        "    n_interactions: int = 600000  # 300 per user avg\n",
        "\n",
        "    # Stronger signal weights\n",
        "    w_view: float = 1.0\n",
        "    w_cart: float = 5.0\n",
        "    w_purchase: float = 15.0\n",
        "\n",
        "    # Temporal split config\n",
        "    val_days: int = 7\n",
        "    test_days: int = 7\n",
        "    k_eval: int = 5\n",
        "\n",
        "    # User behavior concentration\n",
        "    user_preference_strength: float = 0.88\n",
        "    purchase_concentration: float = 0.95\n",
        "\n",
        "cfg = CFG()\n",
        "rng = np.random.default_rng(cfg.seed)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Users: {cfg.n_users}, Items: {cfg.n_items}\")\n",
        "print(f\"  Interactions: {cfg.n_interactions} ({cfg.n_interactions/cfg.n_users:.0f} per user)\")\n",
        "print(f\"  Preference strength: {cfg.user_preference_strength:.0%}\")\n",
        "print(f\"  Purchase concentration: {cfg.purchase_concentration:.0%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c372a0",
      "metadata": {
        "id": "b7c372a0"
      },
      "source": [
        "## 3) Synthetic data generation\n",
        "\n",
        "Generates stronger user-item affinity, with purchases concentrated in primary favorites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd746e1",
      "metadata": {
        "id": "ecd746e1",
        "outputId": "9bcdc5ba-8593-40dc-ef5b-68e10ca03a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Generated data in 17.7s\n",
            "\n",
            " Items: (1000, 4), Interactions: (600000, 4)\n",
            "\n",
            " Items: \n",
            "   item_id category              title  \\\n",
            "0        0   cat_01  vintage monitor 0   \n",
            "1        1   cat_15   vintage laptop 1   \n",
            "2        2   cat_13  vintage speaker 2   \n",
            "3        3   cat_08        pro phone 3   \n",
            "4        4   cat_08     smart camera 4   \n",
            "\n",
            "                                                                              text  \n",
            "0    vintage monitor 0 new arrival. category cat_01. high quality vintage monitor.  \n",
            "1        vintage laptop 1 top rated. category cat_15. high quality vintage laptop.  \n",
            "2  vintage speaker 2 limited edition. category cat_13. high quality vintage spe...  \n",
            "3            pro phone 3 limited edition. category cat_08. high quality pro phone.  \n",
            "4          smart camera 4 new arrival. category cat_08. high quality smart camera.  \n",
            "\n",
            " Interactions: \n",
            "   user_id  item_id event_type   timestamp\n",
            "0      872      518       cart  1700000000\n",
            "1      991      689       view  1700000012\n",
            "2     1576      998   purchase  1700000014\n",
            "3      242      525       view  1700000014\n",
            "4     1819      766       cart  1700000014\n",
            "\n",
            "  Event distribution:\n",
            "event_type\n",
            "view        0.700125\n",
            "cart        0.199830\n",
            "purchase    0.100045\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# 3) Data generation - improved\n",
        "\n",
        "def generate_synthetic_ecommerce(\n",
        "    n_users: int,\n",
        "    n_items: int,\n",
        "    n_interactions: int,\n",
        "    n_categories: int,\n",
        "    seed: int,\n",
        "    user_pref_strength: float = 0.88,\n",
        "    purchase_concentration: float = 0.95,\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Generate data with stronger user-item affinity patterns.\"\"\"\n",
        "    rng_local = np.random.default_rng(seed)\n",
        "\n",
        "    categories = [f\"cat_{i:02d}\" for i in range(n_categories)]\n",
        "    item_cat = rng_local.integers(0, n_categories, size=n_items)\n",
        "\n",
        "    adjective = [\"premium\", \"classic\", \"smart\", \"eco\", \"luxury\", \"pro\", \"elite\", \"modern\", \"vintage\", \"designer\"]\n",
        "    noun = [\"watch\", \"laptop\", \"phone\", \"camera\", \"headphones\", \"speaker\", \"tablet\", \"monitor\", \"keyboard\", \"mouse\"]\n",
        "    tagline = [\"bestseller\", \"trending\", \"new arrival\", \"limited edition\", \"top rated\"]\n",
        "\n",
        "    titles, texts = [], []\n",
        "    for i in range(n_items):\n",
        "        a = adjective[int(rng_local.integers(0, len(adjective)))]\n",
        "        n = noun[int(rng_local.integers(0, len(noun)))]\n",
        "        t = tagline[int(rng_local.integers(0, len(tagline)))]\n",
        "        c = categories[int(item_cat[i])]\n",
        "        title = f\"{a} {n} {i}\"\n",
        "        text = f\"{title} {t}. category {c}. high quality {a} {n}.\"\n",
        "        titles.append(title)\n",
        "        texts.append(text)\n",
        "\n",
        "    items = pd.DataFrame({\n",
        "        \"item_id\": np.arange(n_items, dtype=int),\n",
        "        \"category\": [categories[int(i)] for i in item_cat],\n",
        "        \"title\": titles,\n",
        "        \"text\": texts,\n",
        "    })\n",
        "\n",
        "    # Each user gets 2-3 favorite categories\n",
        "    user_fav = np.zeros((n_users, 3), dtype=int)\n",
        "    for u in range(n_users):\n",
        "        primary = rng_local.integers(0, n_categories)\n",
        "        user_fav[u, 0] = primary\n",
        "        user_fav[u, 1] = (primary + rng_local.integers(1, 5)) % n_categories\n",
        "        user_fav[u, 2] = (primary + rng_local.integers(5, 10)) % n_categories\n",
        "\n",
        "    start_ts = 1700000000\n",
        "    end_ts = start_ts + 60 * 24 * 3600\n",
        "\n",
        "    user_ids = rng_local.integers(0, n_users, size=n_interactions)\n",
        "    timestamps = rng_local.integers(start_ts, end_ts, size=n_interactions)\n",
        "\n",
        "    # More purchases\n",
        "    event_probs = np.array([0.70, 0.20, 0.10])\n",
        "    event_types = rng_local.choice([\"view\", \"cart\", \"purchase\"], size=n_interactions, p=event_probs)\n",
        "\n",
        "    # Item popularity (power law)\n",
        "    item_pop = rng_local.pareto(a=1.5, size=n_items) + 1.0\n",
        "    item_pop = item_pop / item_pop.sum()\n",
        "\n",
        "    chosen_items = np.empty(n_interactions, dtype=int)\n",
        "    for idx in range(n_interactions):\n",
        "        u = int(user_ids[idx])\n",
        "        event = event_types[idx]\n",
        "        fav_cats = user_fav[u]\n",
        "\n",
        "        pref_strength = purchase_concentration if event == \"purchase\" else user_pref_strength\n",
        "\n",
        "        if rng_local.random() < pref_strength:\n",
        "            if event == \"purchase\":\n",
        "                cat = int(fav_cats[0])  # primary favorite for purchases\n",
        "            else:\n",
        "                cat = int(fav_cats[int(rng_local.integers(0, len(fav_cats)))])\n",
        "\n",
        "            candidates = np.where(item_cat == cat)[0]\n",
        "            chosen_items[idx] = int(rng_local.choice(candidates))\n",
        "        else:\n",
        "            chosen_items[idx] = int(rng_local.choice(np.arange(n_items), p=item_pop))\n",
        "\n",
        "    interactions = pd.DataFrame({\n",
        "        \"user_id\": user_ids.astype(int),\n",
        "        \"item_id\": chosen_items.astype(int),\n",
        "        \"event_type\": event_types.astype(str),\n",
        "        \"timestamp\": timestamps.astype(int),\n",
        "    }).sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "    return items, interactions\n",
        "\n",
        "\n",
        "# Generate data\n",
        "t0 = time.time()\n",
        "items, interactions = generate_synthetic_ecommerce(\n",
        "    n_users=cfg.n_users,\n",
        "    n_items=cfg.n_items,\n",
        "    n_interactions=cfg.n_interactions,\n",
        "    n_categories=cfg.n_categories,\n",
        "    seed=cfg.seed,\n",
        "    user_pref_strength=cfg.user_preference_strength,\n",
        "    purchase_concentration=cfg.purchase_concentration,\n",
        ")\n",
        "print(f\"\\n✓ Generated data in {time.time()-t0:.1f}s\")\n",
        "print(f\"\\n Items: {items.shape}, Interactions: {interactions.shape}\")\n",
        "print(f\"\\n Items: \\n{items.head()}\")\n",
        "print(f\"\\n Interactions: \\n{interactions.head()}\")\n",
        "print(f\"\\n  Event distribution:\\n{interactions['event_type'].value_counts(normalize=True)}\")\n",
        "\n",
        "# Add weights + dedupe\n",
        "weight_map = {\"view\": cfg.w_view, \"cart\": cfg.w_cart, \"purchase\": cfg.w_purchase}\n",
        "interactions[\"weight\"] = interactions[\"event_type\"].map(weight_map).astype(float)\n",
        "interactions = interactions.drop_duplicates(subset=[\"user_id\", \"item_id\", \"event_type\", \"timestamp\"]).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "147f286d",
      "metadata": {
        "id": "147f286d"
      },
      "source": [
        "## 4) Temporal split\n",
        "\n",
        "Train, validation, and test are split by time to avoid leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c2a9fe9",
      "metadata": {
        "id": "1c2a9fe9",
        "outputId": "01fa7d71-b971-486f-db80-1c5083369eee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Split: train=460289, val=70007, test=69704\n"
          ]
        }
      ],
      "source": [
        "# 4) Temporal split\n",
        "\n",
        "def temporal_split(interactions: pd.DataFrame, val_days: int, test_days: int) -> Dict[str, pd.DataFrame]:\n",
        "    df = interactions.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    max_ts = int(df[\"timestamp\"].max())\n",
        "    day = 24 * 3600\n",
        "    test_start = max_ts - test_days * day\n",
        "    val_start = test_start - val_days * day\n",
        "\n",
        "    train = df[df[\"timestamp\"] < val_start].copy()\n",
        "    val = df[(df[\"timestamp\"] >= val_start) & (df[\"timestamp\"] < test_start)].copy()\n",
        "    test = df[df[\"timestamp\"] >= test_start].copy()\n",
        "\n",
        "    return {\"train\": train.reset_index(drop=True), \"val\": val.reset_index(drop=True), \"test\": test.reset_index(drop=True)}\n",
        "\n",
        "\n",
        "split = temporal_split(interactions, val_days=cfg.val_days, test_days=cfg.test_days)\n",
        "print(f\"\\n✓ Split: train={split['train'].shape[0]}, val={split['val'].shape[0]}, test={split['test'].shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe4db0e",
      "metadata": {
        "id": "bfe4db0e"
      },
      "source": [
        "## 5) Feature engineering\n",
        "\n",
        "Creates user/item aggregates and recency features used by the hybrid reranker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df006b00",
      "metadata": {
        "id": "df006b00",
        "outputId": "8348f99b-c66b-4dcd-b756-f123dde53aa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Features: 2000 users, 1000 items\n"
          ]
        }
      ],
      "source": [
        "# 5) Feature engineering\n",
        "\n",
        "def build_feature_tables(interactions_train: pd.DataFrame, items: pd.DataFrame, now_ts: int):\n",
        "    df = interactions_train.copy()\n",
        "\n",
        "    user_agg = df.groupby(\"user_id\").agg(\n",
        "        user_events=(\"weight\", \"size\"),\n",
        "        user_weight_sum=(\"weight\", \"sum\"),\n",
        "        user_last_ts=(\"timestamp\", \"max\"),\n",
        "    ).reset_index()\n",
        "    user_agg[\"user_recency_days\"] = (now_ts - user_agg[\"user_last_ts\"]) / (24 * 3600)\n",
        "\n",
        "    item_agg = df.groupby(\"item_id\").agg(\n",
        "        item_events=(\"weight\", \"size\"),\n",
        "        item_weight_sum=(\"weight\", \"sum\"),\n",
        "        item_last_ts=(\"timestamp\", \"max\"),\n",
        "    ).reset_index()\n",
        "    item_agg[\"item_recency_days\"] = (now_ts - item_agg[\"item_last_ts\"]) / (24 * 3600)\n",
        "    item_agg = item_agg.merge(items[[\"item_id\", \"category\"]], on=\"item_id\", how=\"left\")\n",
        "\n",
        "    return user_agg, item_agg\n",
        "\n",
        "\n",
        "now_ts = int(split[\"train\"][\"timestamp\"].max())\n",
        "user_features, item_features = build_feature_tables(split[\"train\"], items, now_ts=now_ts)\n",
        "print(f\"✓ Features: {user_features.shape[0]} users, {item_features.shape[0]} items\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0938f3f",
      "metadata": {
        "id": "e0938f3f"
      },
      "source": [
        "## 6) Models\n",
        "\n",
        "Popularity, Item-Item CF, Content TF-IDF, and Hybrid reranker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6034963f",
      "metadata": {
        "id": "6034963f"
      },
      "outputs": [],
      "source": [
        "# 6) Models\n",
        "\n",
        "class PopularityRecommender:\n",
        "    def __init__(self):\n",
        "        self.global_top: List[Tuple[int, float]] = []\n",
        "        self.by_category_top: Dict[str, List[Tuple[int, float]]] = {}\n",
        "\n",
        "    def fit(self, interactions_train: pd.DataFrame, items: pd.DataFrame):\n",
        "        score = interactions_train.groupby(\"item_id\")[\"weight\"].sum().sort_values(ascending=False)\n",
        "        self.global_top = [(int(i), float(s)) for i, s in score.head(500).items()]\n",
        "\n",
        "        merged = interactions_train.merge(items[[\"item_id\", \"category\"]], on=\"item_id\", how=\"left\")\n",
        "        by_cat = {}\n",
        "        for cat, g in merged.groupby(\"category\"):\n",
        "            s = g.groupby(\"item_id\")[\"weight\"].sum().sort_values(ascending=False)\n",
        "            by_cat[str(cat)] = [(int(i), float(v)) for i, v in s.head(500).items()]\n",
        "        self.by_category_top = by_cat\n",
        "\n",
        "    def recommend(self, user_id: int, k: int, category: Optional[str] = None):\n",
        "        if category and category in self.by_category_top:\n",
        "            return [(i, s, \"popularity_category\") for i, s in self.by_category_top[category][:k]]\n",
        "        return [(i, s, \"popularity_global\") for i, s in self.global_top[:k]]\n",
        "\n",
        "\n",
        "class ItemItemCFRecommender:\n",
        "    def __init__(self, topk_sim: int = 150):\n",
        "        self.topk_sim = topk_sim\n",
        "        self.item_index: Dict[int, int] = {}\n",
        "        self.index_item: Dict[int, int] = {}\n",
        "        self.user_items: Dict[int, np.ndarray] = {}\n",
        "        self.user_weights: Dict[int, np.ndarray] = {}\n",
        "        self.sim_topk: Dict[int, Tuple[np.ndarray, np.ndarray]] = {}\n",
        "\n",
        "    def fit(self, interactions_train: pd.DataFrame):\n",
        "        df = interactions_train[[\"user_id\", \"item_id\", \"weight\"]].copy()\n",
        "        users = df[\"user_id\"].unique()\n",
        "        items_u = df[\"item_id\"].unique()\n",
        "\n",
        "        self.item_index = {int(item_id): idx for idx, item_id in enumerate(items_u)}\n",
        "        self.index_item = {idx: item_id for item_id, idx in self.item_index.items()}\n",
        "\n",
        "        user_index = {int(u): idx for idx, u in enumerate(users)}\n",
        "        ui = df[\"user_id\"].map(user_index).to_numpy()\n",
        "        ii = df[\"item_id\"].map(self.item_index).to_numpy()\n",
        "        ww = df[\"weight\"].to_numpy().astype(np.float32)\n",
        "\n",
        "        X = sparse.csr_matrix((ww, (ui, ii)), shape=(len(users), len(items_u)))\n",
        "        X = normalize(X, norm=\"l2\", axis=1)\n",
        "        S = (X.T @ X).tocsr()\n",
        "\n",
        "        self.sim_topk = {}\n",
        "        for j in range(S.shape[0]):\n",
        "            row = S.getrow(j)\n",
        "            if row.nnz == 0:\n",
        "                self.sim_topk[j] = (np.array([], dtype=int), np.array([], dtype=float))\n",
        "                continue\n",
        "            idxs = row.indices\n",
        "            vals = row.data\n",
        "            mask = idxs != j\n",
        "            idxs = idxs[mask]\n",
        "            vals = vals[mask]\n",
        "            if len(vals) == 0:\n",
        "                self.sim_topk[j] = (np.array([], dtype=int), np.array([], dtype=float))\n",
        "                continue\n",
        "            order = np.argsort(-vals)[:self.topk_sim]\n",
        "            self.sim_topk[j] = (idxs[order].astype(int), vals[order].astype(float))\n",
        "\n",
        "        grouped = df.groupby(\"user_id\").apply(lambda g: (g[\"item_id\"].values, g[\"weight\"].values))\n",
        "        self.user_items = {}\n",
        "        self.user_weights = {}\n",
        "        for u, (its, wts) in grouped.items():\n",
        "            arr = np.array([self.item_index[int(it)] for it in its if int(it) in self.item_index], dtype=int)\n",
        "            wts_arr = np.array([w for it, w in zip(its, wts) if int(it) in self.item_index], dtype=float)\n",
        "            if arr.size > 0:\n",
        "                self.user_items[int(u)] = arr\n",
        "                self.user_weights[int(u)] = wts_arr\n",
        "\n",
        "    def recommend(self, user_id: int, k: int):\n",
        "        if user_id not in self.user_items:\n",
        "            return []\n",
        "        hist = self.user_items[user_id]\n",
        "        weights = self.user_weights[user_id]\n",
        "\n",
        "        recency_boost = np.linspace(0.5, 1.5, len(hist))\n",
        "        weights = weights * recency_boost\n",
        "\n",
        "        scores = {}\n",
        "        recent_window = min(100, len(hist))\n",
        "        for it, w in zip(hist[-recent_window:], weights[-recent_window:]):\n",
        "            neigh_idx, neigh_sim = self.sim_topk.get(int(it), (None, None))\n",
        "            if neigh_idx is None:\n",
        "                continue\n",
        "            for n, s in zip(neigh_idx, neigh_sim):\n",
        "                scores[int(n)] = scores.get(int(n), 0.0) + float(s) * float(w)\n",
        "\n",
        "        hist_set = set(int(x) for x in hist.tolist())\n",
        "        items_scored = [(i, s) for i, s in scores.items() if i not in hist_set]\n",
        "        items_scored.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return [(int(self.index_item[idx]), float(s), \"cf_item_item\") for idx, s in items_scored[:k]]\n",
        "\n",
        "\n",
        "class ContentTFIDFRecommender:\n",
        "    def __init__(self, max_features: int = 8000):\n",
        "        self.max_features = max_features\n",
        "        self.vectorizer = None\n",
        "        self.item_ids = None\n",
        "        self.item_matrix = None\n",
        "        self.item_index = {}\n",
        "\n",
        "    def fit(self, items: pd.DataFrame):\n",
        "        self.item_ids = items[\"item_id\"].to_numpy().astype(int)\n",
        "        texts = items[\"text\"].astype(str).tolist()\n",
        "        self.vectorizer = TfidfVectorizer(max_features=self.max_features, ngram_range=(1, 2))\n",
        "        X = self.vectorizer.fit_transform(texts)\n",
        "        X = normalize(X, norm=\"l2\", axis=1)\n",
        "        self.item_matrix = X.tocsr()\n",
        "        self.item_index = {int(item_id): idx for idx, item_id in enumerate(self.item_ids.tolist())}\n",
        "\n",
        "    def similar_items(self, item_id: int, k: int):\n",
        "        if self.item_matrix is None or item_id not in self.item_index:\n",
        "            return []\n",
        "        i = self.item_index[item_id]\n",
        "        q = self.item_matrix.getrow(i)\n",
        "        sims = (self.item_matrix @ q.T).toarray().ravel()\n",
        "        sims[i] = -1.0\n",
        "        top = np.argsort(-sims)[:k]\n",
        "        return [(int(self.item_ids[j]), float(sims[j])) for j in top]\n",
        "\n",
        "\n",
        "class HybridReranker:\n",
        "    def __init__(\n",
        "        self,\n",
        "        pop,\n",
        "        cf,\n",
        "        content,\n",
        "        user_features,\n",
        "        item_features,\n",
        "        items,\n",
        "        seed,\n",
        "        candidates_per_model=350,\n",
        "        rerank_topn=120,\n",
        "        neg_per_pos=3,\n",
        "    ):\n",
        "        self.pop = pop\n",
        "        self.cf = cf\n",
        "        self.content = content\n",
        "        self.user_features = user_features\n",
        "        self.item_features = item_features\n",
        "        self.items = items[[\"item_id\", \"category\"]].copy()\n",
        "        self.seed = seed\n",
        "        self.candidates_per_model = candidates_per_model\n",
        "        self.rerank_topn = rerank_topn\n",
        "        self.neg_per_pos = neg_per_pos\n",
        "        self.model = LogisticRegression(max_iter=500, random_state=seed, C=0.5, class_weight=\"balanced\")\n",
        "        self.user_hist = {}\n",
        "        self.user_categories = {}\n",
        "\n",
        "    def set_user_history(self, interactions_train: pd.DataFrame):\n",
        "        hist = interactions_train.sort_values(\"timestamp\").groupby(\"user_id\")[\"item_id\"].apply(list)\n",
        "        self.user_hist = {int(u): [int(x) for x in lst] for u, lst in hist.items()}\n",
        "        merged = interactions_train.merge(self.items, on=\"item_id\", how=\"left\")\n",
        "        user_cats = merged.groupby(\"user_id\")[\"category\"].apply(lambda x: x.mode()[0] if len(x) > 0 else None)\n",
        "        self.user_categories = {int(u): str(c) for u, c in user_cats.items() if c is not None}\n",
        "\n",
        "    def _user_row(self, user_id: int):\n",
        "        row = self.user_features[self.user_features[\"user_id\"] == user_id]\n",
        "        if len(row) == 0:\n",
        "            return {\"user_events\": 0.0, \"user_weight_sum\": 0.0, \"user_recency_days\": 999.0}\n",
        "        r = row.iloc[0]\n",
        "        return {\n",
        "            \"user_events\": float(r[\"user_events\"]),\n",
        "            \"user_weight_sum\": float(r[\"user_weight_sum\"]),\n",
        "            \"user_recency_days\": float(r[\"user_recency_days\"]),\n",
        "        }\n",
        "\n",
        "    def _item_row(self, item_id: int):\n",
        "        row = self.item_features[self.item_features[\"item_id\"] == item_id]\n",
        "        if len(row) == 0:\n",
        "            return {\"item_events\": 0.0, \"item_weight_sum\": 0.0, \"item_recency_days\": 999.0}\n",
        "        r = row.iloc[0]\n",
        "        return {\n",
        "            \"item_events\": float(r[\"item_events\"]),\n",
        "            \"item_weight_sum\": float(r[\"item_weight_sum\"]),\n",
        "            \"item_recency_days\": float(r[\"item_recency_days\"]),\n",
        "        }\n",
        "\n",
        "    def _featurize(self, user_id: int, item_id: int, cf_score: float, content_score: float):\n",
        "        u = self._user_row(user_id)\n",
        "        it = self._item_row(item_id)\n",
        "\n",
        "        item_row = self.items[self.items[\"item_id\"] == item_id]\n",
        "        item_cat = item_row[\"category\"].iloc[0] if len(item_row) > 0 else \"\"\n",
        "        user_fav_cat = self.user_categories.get(user_id, \"\")\n",
        "        category_match = 1.0 if item_cat == user_fav_cat else 0.0\n",
        "\n",
        "        return np.array([\n",
        "            u[\"user_events\"],\n",
        "            u[\"user_weight_sum\"],\n",
        "            np.log1p(u[\"user_weight_sum\"]),\n",
        "            u[\"user_recency_days\"],\n",
        "            it[\"item_events\"],\n",
        "            it[\"item_weight_sum\"],\n",
        "            np.log1p(it[\"item_weight_sum\"]),\n",
        "            it[\"item_recency_days\"],\n",
        "            cf_score,\n",
        "            np.log1p(cf_score),\n",
        "            content_score,\n",
        "            category_match,\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def _build_candidates(self, user_id: int):\n",
        "        candidates = {}\n",
        "\n",
        "        cf_recs = self.cf.recommend(user_id, self.candidates_per_model)\n",
        "        for item_id, score, _ in cf_recs:\n",
        "            candidates[int(item_id)] = (float(score), 0.0)\n",
        "\n",
        "        content_candidates = []\n",
        "        if user_id in self.user_hist and len(self.user_hist[user_id]) > 0:\n",
        "            for last_item_id in self.user_hist[user_id][-3:]:\n",
        "                content_candidates.extend(self.content.similar_items(last_item_id, self.candidates_per_model // 3))\n",
        "\n",
        "        for item_id, score in content_candidates:\n",
        "            if int(item_id) in candidates:\n",
        "                candidates[int(item_id)] = (candidates[int(item_id)][0], float(score))\n",
        "            else:\n",
        "                candidates[int(item_id)] = (0.0, float(score))\n",
        "\n",
        "        return [(it, cfs, cos) for it, (cfs, cos) in candidates.items()]\n",
        "\n",
        "    def fit(self, interactions_train: pd.DataFrame, interactions_val: pd.DataFrame):\n",
        "        rng_local = np.random.default_rng(self.seed)\n",
        "        val = interactions_val.copy()\n",
        "        pos = val[val[\"event_type\"] == \"purchase\"][[\"user_id\", \"item_id\"]].drop_duplicates()\n",
        "        if len(pos) < 1000:\n",
        "            cart_pos = val[val[\"event_type\"] == \"cart\"][[\"user_id\", \"item_id\"]].drop_duplicates()\n",
        "            pos = pd.concat([pos, cart_pos]).drop_duplicates()\n",
        "\n",
        "        X_rows, y_rows = [], []\n",
        "        users = pos[\"user_id\"].astype(int).unique().tolist()[:2000]\n",
        "        for u in users:\n",
        "            u_pos_items = pos[pos[\"user_id\"] == u][\"item_id\"].astype(int).tolist()\n",
        "            if not u_pos_items:\n",
        "                continue\n",
        "\n",
        "            cand = self._build_candidates(u)\n",
        "            if not cand:\n",
        "                continue\n",
        "\n",
        "            cand_map = {it: (cfs, cos) for it, cfs, cos in cand}\n",
        "\n",
        "            for it in u_pos_items[:10]:\n",
        "                cfs, cos = cand_map.get(int(it), (0.0, 0.0))\n",
        "                X_rows.append(self._featurize(u, int(it), cfs, cos))\n",
        "                y_rows.append(1)\n",
        "\n",
        "                cand_items = [c for c in cand_map.keys() if c not in set(u_pos_items)]\n",
        "                n_negs = min(self.neg_per_pos, len(cand_items))\n",
        "                if n_negs > 0:\n",
        "                    neg_items = rng_local.choice(cand_items, size=n_negs, replace=False)\n",
        "                    for neg_it in neg_items:\n",
        "                        cfs2, cos2 = cand_map.get(int(neg_it), (0.0, 0.0))\n",
        "                        X_rows.append(self._featurize(u, int(neg_it), cfs2, cos2))\n",
        "                        y_rows.append(0)\n",
        "\n",
        "        X = np.vstack(X_rows) if X_rows else np.zeros((0, 12), dtype=np.float32)\n",
        "        y = np.array(y_rows, dtype=int)\n",
        "\n",
        "        if len(np.unique(y)) < 2 or len(X) < 100:\n",
        "            X = np.random.default_rng(self.seed).normal(size=(500, 12)).astype(np.float32)\n",
        "            y = np.array([0] * 250 + [1] * 250, dtype=int)\n",
        "\n",
        "        print(f\"  Training reranker on {len(X)} examples ({sum(y)} positive)\")\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "    def recommend(self, user_id: int, k: int):\n",
        "        cand = self._build_candidates(user_id)\n",
        "        if not cand:\n",
        "            return []\n",
        "\n",
        "        X = np.vstack([self._featurize(user_id, it, cfs, cos) for it, cfs, cos in cand]).astype(np.float32)\n",
        "        proba = self.model.predict_proba(X)[:, 1]\n",
        "        order = np.argsort(-proba)[:self.rerank_topn]\n",
        "        return [(int(cand[int(idx)][0]), float(proba[int(idx)]), \"hybrid_rerank\") for idx in order[:k]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a07c5c20",
      "metadata": {
        "id": "a07c5c20"
      },
      "source": [
        "## 7) Training\n",
        "\n",
        "Fits popularity, CF, content, and hybrid reranker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86eed0a9",
      "metadata": {
        "id": "86eed0a9",
        "outputId": "6e789ace-bf84-4d19-c3b6-a6bd8639d134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING MODELS\n",
            "============================================================\n",
            "✓ Popularity trained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2964323340.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  grouped = df.groupby(\"user_id\").apply(lambda g: (g[\"item_id\"].values, g[\"weight\"].values))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ CF Item-Item trained\n",
            "✓ Content TF-IDF trained\n",
            "  Training reranker on 27216 examples (6804 positive)\n",
            "✓ Hybrid trained\n",
            "\n",
            "Total training time: 77.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# 7) Training\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "pop = PopularityRecommender()\n",
        "pop.fit(split[\"train\"], items)\n",
        "print(\"✓ Popularity trained\")\n",
        "\n",
        "cf = ItemItemCFRecommender(topk_sim=150)\n",
        "cf.fit(split[\"train\"])\n",
        "print(\"✓ CF Item-Item trained\")\n",
        "\n",
        "content = ContentTFIDFRecommender(max_features=8000)\n",
        "content.fit(items)\n",
        "print(\"✓ Content TF-IDF trained\")\n",
        "\n",
        "hybrid = HybridReranker(\n",
        "    pop, cf, content, user_features, item_features, items,\n",
        "    seed=cfg.seed, candidates_per_model=350, rerank_topn=120, neg_per_pos=3\n",
        ")\n",
        "hybrid.set_user_history(split[\"train\"])\n",
        "hybrid.fit(split[\"train\"], split[\"val\"])\n",
        "print(\"✓ Hybrid trained\")\n",
        "\n",
        "print(f\"\\nTotal training time: {time.time()-t0:.1f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19cef4ad",
      "metadata": {
        "id": "19cef4ad"
      },
      "source": [
        "## 8) Evaluation metrics\n",
        "\n",
        "Precision, recall, NDCG, MAP, and coverage utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26f3e197",
      "metadata": {
        "id": "26f3e197"
      },
      "outputs": [],
      "source": [
        "# 8) Evaluation metrics\n",
        "\n",
        "def precision_at_k(ranked: List[int], relevant: Set[int], k: int):\n",
        "    if k <= 0:\n",
        "        return 0.0\n",
        "    top = ranked[:k]\n",
        "    return sum(1 for x in top if x in relevant) / float(k) if top else 0.0\n",
        "\n",
        "def recall_at_k(ranked: List[int], relevant: Set[int], k: int):\n",
        "    if not relevant:\n",
        "        return 0.0\n",
        "    top = ranked[:k]\n",
        "    return sum(1 for x in top if x in relevant) / float(len(relevant))\n",
        "\n",
        "def ndcg_at_k(ranked: List[int], relevant: Set[int], k: int):\n",
        "    if not relevant:\n",
        "        return 0.0\n",
        "    dcg = sum(1.0 / math.log(i + 2, 2) for i, item in enumerate(ranked[:k]) if item in relevant)\n",
        "    idcg = sum(1.0 / math.log(i + 2, 2) for i in range(min(len(relevant), k)))\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def average_precision_at_k(ranked: List[int], relevant: Set[int], k: int):\n",
        "    if not relevant:\n",
        "        return 0.0\n",
        "    ap = 0.0\n",
        "    hits = 0\n",
        "    for i, item in enumerate(ranked[:k], start=1):\n",
        "        if item in relevant:\n",
        "            hits += 1\n",
        "            ap += hits / float(i)\n",
        "    return ap / float(min(len(relevant), k))\n",
        "\n",
        "def ground_truth(interactions_test: pd.DataFrame):\n",
        "    df = interactions_test[interactions_test[\"event_type\"].isin([\"purchase\", \"cart\"])][[\"user_id\", \"item_id\"]].drop_duplicates()\n",
        "    gt = {}\n",
        "    for u, g in df.groupby(\"user_id\"):\n",
        "        gt[int(u)] = set(int(x) for x in g[\"item_id\"].tolist())\n",
        "    return gt\n",
        "\n",
        "def coverage(recs_by_user: Dict[int, List[int]], catalog_size: int):\n",
        "    if catalog_size <= 0:\n",
        "        return 0.0\n",
        "    s = set()\n",
        "    for lst in recs_by_user.values():\n",
        "        s.update(lst)\n",
        "    return len(s) / float(catalog_size)\n",
        "\n",
        "def evaluate_model(model_name: str, model_obj, k: int, max_users: int = 1000):\n",
        "    gt = ground_truth(split[\"test\"])\n",
        "    users = sorted(gt.keys())\n",
        "    if len(users) > max_users:\n",
        "        users = rng.choice(users, size=max_users, replace=False).tolist()\n",
        "\n",
        "    recs_by_user = {}\n",
        "    p_list, r_list, n_list, m_list = [], [], [], []\n",
        "\n",
        "    for u in users:\n",
        "        recs = model_obj.recommend(int(u), k=k)\n",
        "        ranked = [int(it) for it, _, _ in recs]\n",
        "        recs_by_user[int(u)] = ranked\n",
        "        relevant = gt.get(int(u), set())\n",
        "        p_list.append(precision_at_k(ranked, relevant, k))\n",
        "        r_list.append(recall_at_k(ranked, relevant, k))\n",
        "        n_list.append(ndcg_at_k(ranked, relevant, k))\n",
        "        m_list.append(average_precision_at_k(ranked, relevant, k))\n",
        "\n",
        "    cov = coverage(recs_by_user, catalog_size=int(items[\"item_id\"].nunique()))\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        f\"precision@{k}\": float(np.mean(p_list)),\n",
        "        f\"recall@{k}\": float(np.mean(r_list)),\n",
        "        f\"ndcg@{k}\": float(np.mean(n_list)),\n",
        "        f\"map@{k}\": float(np.mean(m_list)),\n",
        "        \"coverage\": cov,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96ad2cbe",
      "metadata": {
        "id": "96ad2cbe"
      },
      "source": [
        "## 9) Evaluation on test set\n",
        "\n",
        "Runs all baselines and hybrid, producing a results table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f4b4f6b",
      "metadata": {
        "id": "8f4b4f6b",
        "outputId": "a639f2ef-0d90-4faa-ed6e-fa47001f0b49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATION ON TEST SET\n",
            "============================================================\n",
            "\n",
            "RESULTS:\n",
            "        model  precision@5  recall@5   ndcg@5    map@5  coverage\n",
            "       hybrid       0.0992  0.048329 0.099080 0.050753     0.957\n",
            " cf_item_item       0.0898  0.044359 0.092676 0.048217     0.812\n",
            "   popularity       0.0390  0.019798 0.048415 0.025834     0.005\n",
            "content_tfidf       0.0130  0.006510 0.015900 0.008290     0.913\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 9) Evaluation on test set\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "k = cfg.k_eval\n",
        "\n",
        "class ContentOnlyWrapper:\n",
        "    def __init__(self, train_interactions, content_model, pop_model):\n",
        "        self.content = content_model\n",
        "        self.pop = pop_model\n",
        "        hist = train_interactions.sort_values(\"timestamp\").groupby(\"user_id\")[\"item_id\"].apply(list)\n",
        "        self.user_hist = {int(u): [int(x) for x in lst] for u, lst in hist.items()}\n",
        "\n",
        "    def recommend(self, user_id: int, k: int):\n",
        "        if user_id not in self.user_hist or len(self.user_hist[user_id]) == 0:\n",
        "            return self.pop.recommend(user_id, k)\n",
        "        last_item_id = int(self.user_hist[user_id][-1])\n",
        "        sims = self.content.similar_items(last_item_id, k)\n",
        "        return [(it, s, \"content_sim\") for it, s in sims] if sims else self.pop.recommend(user_id, k)\n",
        "\n",
        "content_only = ContentOnlyWrapper(split[\"train\"], content, pop)\n",
        "\n",
        "rows = [\n",
        "    evaluate_model(\"popularity\", pop, k),\n",
        "    evaluate_model(\"cf_item_item\", cf, k),\n",
        "    evaluate_model(\"content_tfidf\", content_only, k),\n",
        "    evaluate_model(\"hybrid\", hybrid, k),\n",
        "]\n",
        "\n",
        "df_results = pd.DataFrame(rows).sort_values(f\"ndcg@{k}\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nRESULTS:\")\n",
        "print(df_results.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "392e26c7",
      "metadata": {
        "id": "392e26c7"
      },
      "source": [
        "## 10) Demo recommendations\n",
        "\n",
        "Shows example recommendations for an active user and CF similar-items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1302f77",
      "metadata": {
        "id": "a1302f77",
        "outputId": "83a5f1ec-b919-4d21-9148-7c9e2c89b70e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "RECOMMENDATIONS FOR USER\n",
            "============================================================\n",
            "\n",
            "User ID: 51\n",
            "Training interactions: 273\n",
            "\n",
            "--- HYBRID RECOMMENDATIONS ---\n",
            " item_id    product_name category    score        reason\n",
            "     608  Elite Keyboard   cat_19 0.922007 hybrid_rerank\n",
            "     946  Designer Mouse   cat_19 0.921287 hybrid_rerank\n",
            "     140  Designer Mouse   cat_19 0.919251 hybrid_rerank\n",
            "     809 Vintage Monitor   cat_19 0.918714 hybrid_rerank\n",
            "     796    Smart Tablet   cat_19 0.917428 hybrid_rerank\n"
          ]
        }
      ],
      "source": [
        "# 10) Demo recommendations\n",
        "\n",
        "item_lookup = {\n",
        "    int(r.item_id): {\"title\": str(r.title), \"category\": str(r.category)}\n",
        "    for r in items[[\"item_id\", \"title\", \"category\"]].itertuples(index=False)\n",
        "}\n",
        "\n",
        "def pretty_recommendations(recs: List[Tuple[int, float, str]]):\n",
        "    out = []\n",
        "    for item_id, score, reason in recs:\n",
        "        meta = item_lookup.get(int(item_id), {\"title\": f\"item_{item_id}\", \"category\": \"unknown\"})\n",
        "        title = meta[\"title\"]\n",
        "        title_parts = title.rsplit(' ', 1)\n",
        "        clean_title = title_parts[0].title() if len(title_parts) == 2 and title_parts[1].isdigit() else title.title()\n",
        "        out.append({\n",
        "            \"item_id\": int(item_id),\n",
        "            \"product_name\": clean_title,\n",
        "            \"category\": meta[\"category\"],\n",
        "            \"score\": float(score),\n",
        "            \"reason\": reason\n",
        "        })\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "def recommend(user_id: int, k: int = 10, model: str = \"hybrid\"):\n",
        "    if model == \"hybrid\":\n",
        "        recs = hybrid.recommend(user_id, k)\n",
        "        if len(recs) < k:\n",
        "            extra = pop.recommend(user_id, k - len(recs))\n",
        "            existing = set(it for it, _, _ in recs)\n",
        "            extra = [(it, s, r) for it, s, r in extra if it not in existing]\n",
        "            recs = recs + extra[:(k - len(recs))]\n",
        "    elif model == \"popularity\":\n",
        "        recs = pop.recommend(user_id, k)\n",
        "    elif model == \"cf\":\n",
        "        recs = cf.recommend(user_id, k)\n",
        "        if len(recs) < k:\n",
        "            extra = pop.recommend(user_id, k - len(recs))\n",
        "            existing = set(it for it, _, _ in recs)\n",
        "            extra = [(it, s, r) for it, s, r in extra if it not in existing]\n",
        "            recs = recs + extra[:(k - len(recs))]\n",
        "    elif model == \"content\":\n",
        "        recs = content_only.recommend(user_id, k)\n",
        "    else:\n",
        "        raise ValueError(\"model must be one of: hybrid, popularity, cf, content\")\n",
        "    return pretty_recommendations(recs)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RECOMMENDATIONS FOR USER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "demo_user = int(split[\"train\"][\"user_id\"].value_counts().index[5])\n",
        "print(f\"\\nUser ID: {demo_user}\")\n",
        "print(f\"Training interactions: {split['train'][split['train']['user_id']==demo_user].shape[0]}\")\n",
        "\n",
        "print(\"\\n--- HYBRID RECOMMENDATIONS ---\")\n",
        "print(recommend(demo_user, k=5, model=\"hybrid\").to_string(index=False))\n",
        "\n",
        "# print(\"\\n--- CF ITEM-ITEM RECOMMENDATIONS ---\")\n",
        "# print(recommend(demo_user, k=5, model=\"cf\").to_string(index=False))\n",
        "\n",
        "# print(\"\\n\" + \"=\"*60)\n",
        "# print(\"DEMO: SIMILAR ITEMS (CF)\")\n",
        "# print(\"=\"*60)\n",
        "\n",
        "# def cf_similar_items(cf_model, item_id: int, k: int = 10):\n",
        "#     item_id = int(item_id)\n",
        "#     if item_id not in cf_model.item_index:\n",
        "#         return []\n",
        "#     j = int(cf_model.item_index[item_id])\n",
        "#     neigh_idx, neigh_sim = cf_model.sim_topk.get(j, (np.array([], dtype=int), np.array([], dtype=float)))\n",
        "#     return [\n",
        "#         (int(cf_model.index_item[int(idx)]), float(s), \"cf_similarity\")\n",
        "#         for idx, s in zip(neigh_idx[:k], neigh_sim[:k])\n",
        "#     ]\n",
        "\n",
        "# last_item = int(split[\"train\"].query(\"user_id == @demo_user\").sort_values(\"timestamp\")[\"item_id\"].iloc[-1])\n",
        "# print(f\"\\nMost recent item in user history: {last_item}\")\n",
        "# print(f\"Product: {item_lookup[last_item]['title']}\")\n",
        "\n",
        "# print(\"\\nSimilar items (CF):\")\n",
        "# print(\n",
        "#     pretty_recommendations(cf_similar_items(cf, last_item, k=5))[\n",
        "#         [\"item_id\", \"product_name\", \"category\", \"score\"]\n",
        "#     ].to_string(index=False)\n",
        "# )\n",
        "\n",
        "# print(\"\\n\" + \"=\"*60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d6b1ec",
      "metadata": {
        "id": "78d6b1ec"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}